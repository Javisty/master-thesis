
@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs}: {A} framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {0004-3702},
	shorttitle = {Between {MDPs} and semi-{MDPs}},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
	doi = {10.1016/S0004-3702(99)00052-1},
	abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.},
	language = {en},
	number = {1},
	urldate = {2022-04-12},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	month = aug,
	year = {1999},
	keywords = {Reinforcement learning, Markov decision processes, Hierarchical planning, Intra-option learning, Macroactions, Macros, Options, Semi-Markov decision processes, Subgoals, Temporal abstraction},
	pages = {181--211},
	file = {Full Text:/home/javisty/Zotero/storage/ZZAFKZBC/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf:application/pdf;ScienceDirect Snapshot:/home/javisty/Zotero/storage/9FF58MMK/S0004370299000521.html:text/html},
}

@inproceedings{sorg_linear_2010,
	title = {Linear options},
	doi = {10.1145/1838206.1838211},
	abstract = {This work develops a knowledge construct, the linear option, which is capable of modeling temporally abstract dynamics in continuous state spaces and shows conditions under which a linear feature set is sufficient for accurately representing the value function of an option policy. Learning, planning, and representing knowledge in large state spaces at multiple levels of temporal abstraction are key, long-standing challenges for building flexible autonomous agents. The options framework provides a formal mechanism for specifying and learning temporally-extended skills. Although past work has demonstrated the benefit of acting according to options in continuous state spaces, one of the central advantages of temporal abstraction---the ability to plan using a temporally abstract model---remains a challenging problem when the number of environment states is large or infinite. In this work, we develop a knowledge construct, the linear option, which is capable of modeling temporally abstract dynamics in continuous state spaces. We show that planning with a linear expectation model of an option's dynamics converges to a fixed point with low Temporal Difference (TD) error. Next, building on recent work on linear feature selection, we show conditions under which a linear feature set is sufficient for accurately representing the value function of an option policy. We extend this result to show conditions under which multiple options may be repeatedly composed to create new options with accurate linear models. Finally, we demonstrate linear option learning and planning algorithms in a simulated robot environment.},
	booktitle = {{AAMAS}},
	author = {Sorg, Jonathan and Singh, Satinder},
	year = {2010},
}

@article{bacon_option-critic_2016,
	title = {The {Option}-{Critic} {Architecture}},
	url = {http://arxiv.org/abs/1609.05140},
	abstract = {Temporal abstraction is key to scaling up learning and planning in reinforcement learning. While planning with temporally extended actions is well understood, creating such abstractions autonomously from data has remained challenging. We tackle this problem in the framework of options [Sutton, Precup \& Singh, 1999; Precup, 2000]. We derive policy gradient theorems for options and propose a new option-critic architecture capable of learning both the internal policies and the termination conditions of options, in tandem with the policy over options, and without the need to provide any additional rewards or subgoals. Experimental results in both discrete and continuous environments showcase the flexibility and efficiency of the framework.},
	urldate = {2022-04-12},
	journal = {arXiv:1609.05140 [cs]},
	author = {Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
	month = dec,
	year = {2016},
	note = {arXiv: 1609.05140
version: 2},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted to the Thirthy-first AAAI Conference On Artificial Intelligence (AAAI), 2017},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/HYSQ3QCL/Bacon et al. - 2016 - The Option-Critic Architecture.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/VEE9QAPH/1609.html:text/html},
}

@article{dietterich_hierarchical_2000,
	title = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
	volume = {13},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10266},
	doi = {10.1613/jair.639},
	abstract = {This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process  MDP  into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics{\textbar}as a subroutine hierarchy{\textbar}and a declarative semantics{\textbar}as a representation of the value function of a hierarchical policy. MAXQ uni es and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals. By de ning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper de nes the MAXQ hierarchy, proves formal results on its representational power, and establishes ve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the ve kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q  with state abstractions  converges to a recursively optimal policy much faster than at Q learning. The fact that MAXQ learns a representation of the value function has an important bene t: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the e ectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeo s in hierarchical reinforcement learning.},
	language = {en},
	urldate = {2022-04-20},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dietterich, T. G.},
	month = nov,
	year = {2000},
	pages = {227--303},
	file = {Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf:/home/javisty/Zotero/storage/NFBYX7BH/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf:application/pdf},
}

@article{jong_utility_2008,
	title = {The {Utility} of {Temporal} {Abstraction} in {Reinforcement} {Learning}},
	abstract = {The hierarchical structure of real-world problems has motivated extensive research into temporal abstractions for reinforcement learning, but precisely how these abstractions allow agents to improve their learning performance is not well understood. This paper investigates the connection between temporal abstraction and an agent’s exploration policy, which determines how the agent’s performance improves over time. Experimental results with standard methods for incorporating temporal abstractions show that these methods beneﬁt learning only in limited contexts. The primary contribution of this paper is a clearer understanding of how hierarchical decompositions interact with reinforcement learning algorithms, with important consequences for the manual design or automatic discovery of action hierarchies.},
	language = {en},
	author = {Jong, Nicholas K and Hester, Todd and Stone, Peter},
	year = {2008},
	pages = {8},
	file = {Jong et al. - The Utility of Temporal Abstraction in Reinforceme.pdf:/home/javisty/Zotero/storage/E8L6RHMD/Jong et al. - The Utility of Temporal Abstraction in Reinforceme.pdf:application/pdf},
}

@inproceedings{jonsson_causal_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {A causal approach to hierarchical decomposition of factored {MDPs}},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102402},
	doi = {10.1145/1102351.1102402},
	abstract = {We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended actions that cause the values of state variables to change. Each temporally-extended action corresponds to a subtask that is significantly easier to solve than the overall task. Results from experiments show great promise in scaling to larger tasks.},
	urldate = {2022-04-20},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Jonsson, Anders and Barto, Andrew},
	month = aug,
	year = {2005},
	pages = {401--408},
	file = {Full Text PDF:/home/javisty/Zotero/storage/LZAGTATC/Jonsson and Barto - 2005 - A causal approach to hierarchical decomposition of.pdf:application/pdf},
}

@inproceedings{nachum_data-efficient_2018,
	title = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
	abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
	urldate = {2022-04-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
	year = {2018},
	file = {Full Text PDF:/home/javisty/Zotero/storage/S7Q6KX88/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf:application/pdf},
}

@article{precup_temporal_2000,
	title = {Temporal abstraction in reinforcement learning},
	url = {https://scholarworks.umass.edu/dissertations/AAI9978540},
	journal = {Doctoral Dissertations Available from Proquest},
	author = {Precup, Doina},
	month = jan,
	year = {2000},
	pages = {1--131},
	file = {"Temporal abstraction in reinforcement learning" by Doina Precup:/home/javisty/Zotero/storage/HGYU7BT9/AAI9978540.html:text/html},
}

@article{harb_when_2017,
	title = {When {Waiting} is not an {Option} : {Learning} {Options} with a {Deliberation} {Cost}},
	shorttitle = {When {Waiting} is not an {Option}},
	url = {http://arxiv.org/abs/1709.04571},
	abstract = {Recent work has shown that temporally extended actions (options) can be learned fully end-to-end as opposed to being specified in advance. While the problem of "how" to learn options is increasingly well understood, the question of "what" good options should be has remained elusive. We formulate our answer to what "good" options should be in the bounded rationality framework (Simon, 1957) through the notion of deliberation cost. We then derive practical gradient-based learning algorithms to implement this objective. Our results in the Arcade Learning Environment (ALE) show increased performance and interpretability.},
	urldate = {2022-04-20},
	journal = {arXiv:1709.04571 [cs]},
	author = {Harb, Jean and Bacon, Pierre-Luc and Klissarov, Martin and Precup, Doina},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04571},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/88A9KCAL/Harb et al. - 2017 - When Waiting is not an Option  Learning Options w.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/SZP5SC3J/1709.html:text/html},
}

@article{alexander_strategic_2016,
	title = {Strategic {Attentive} {Writer} for {Learning} {Macro}-{Actions}},
	url = {http://arxiv.org/abs/1606.04695},
	abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
	urldate = {2022-04-20},
	journal = {arXiv:1606.04695 [cs]},
	author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04695},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8JKQ58PQ/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Acti.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/R7KNETYY/1606.html:text/html},
}

@article{machado_temporal_2021,
	title = {Temporal {Abstraction} in {Reinforcement} {Learning} with the {Successor} {Representation}},
	url = {http://arxiv.org/abs/2110.05740},
	abstract = {Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence. In reinforcement learning, this is often modeled through temporally extended courses of actions called options. Options allow agents to make predictions and to operate at different levels of abstraction within an environment. Nevertheless, approaches based on the options framework often start with the assumption that a reasonable set of options is known beforehand. When this is not the case, there are no definitive answers for which options one should consider. In this paper, we argue that the successor representation (SR), which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions. To support our claim, we take a big picture view of recent results, showing how the SR can be used to discover options that facilitate either temporally-extended exploration or planning. We cast these results as instantiations of a general framework for option discovery in which the agent's representation is used to identify useful options, which are then used to further improve its representation. This results in a virtuous, never-ending, cycle in which both the representation and the options are constantly refined based on each other. Beyond option discovery itself, we discuss how the SR allows us to augment a set of options into a combinatorially large counterpart without additional learning. This is achieved through the combination of previously learned options. Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the SR to combine them. The results of our experiments shed light on design decisions involved in the definition of options and demonstrate the synergy of different methods based on the SR, such as eigenoptions and the option keyboard.},
	urldate = {2022-04-20},
	journal = {arXiv:2110.05740 [cs]},
	author = {Machado, Marlos C. and Barreto, Andre and Precup, Doina},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.05740},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: 56 pages, 28 figures},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/EMQRWULZ/Machado et al. - 2021 - Temporal Abstraction in Reinforcement Learning wit.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/NBDF6TGY/2110.html:text/html},
}

@inproceedings{mann_time-regularized_2014,
	title = {Time-{Regularized} {Interrupting} {Options} ({TRIO})},
	url = {https://proceedings.mlr.press/v32/mannb14.html},
	abstract = {High-level skills relieve planning algorithms from low-level details. But when the skills are poorly designed for the domain, the resulting plan may be severely suboptimal. Sutton et al. 1999 made an important step towards resolving this problem by introducing a rule that automatically improves a set of skills called options. This rule terminates an option early whenever switching to another option gives a higher value than continuing with the current option. However, they only analyzed the case where the improvement rule is applied once. We show conditions where this rule converges to the optimal set of options. A new Bellman-like operator that simultaneously improves the set of options is at the core of our analysis. One problem with the update rule is that it tends to favor lower-level skills. Therefore we introduce a regularization term that favors longer duration skills. Experimental results demonstrate that this approach can derive a good set of high-level skills even when the original set of skills cannot solve the problem.},
	language = {en},
	urldate = {2022-04-26},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mann, Timothy and Mankowitz, Daniel and Mannor, Shie},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {1350--1358},
	file = {Full Text PDF:/home/javisty/Zotero/storage/M8GZAW6W/Mann et al. - 2014 - Time-Regularized Interrupting Options (TRIO).pdf:application/pdf},
}

@article{nachum_why_2019,
	title = {Why {Does} {Hierarchy} ({Sometimes}) {Work} {So} {Well} in {Reinforcement} {Learning}?},
	url = {http://arxiv.org/abs/1909.10618},
	abstract = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},
	urldate = {2022-04-28},
	journal = {arXiv:1909.10618 [cs, stat]},
	author = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.10618},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented as an oral at the NeurIPS 2019 DeepRL Workshop},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/R9Y6KQ2E/Nachum et al. - 2019 - Why Does Hierarchy (Sometimes) Work So Well in Rei.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/I53ULGJY/1909.html:text/html},
}

@inproceedings{stolle_learning_2002,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Options} in {Reinforcement} {Learning}},
	isbn = {978-3-540-45622-3},
	doi = {10.1007/3-540-45622-8_16},
	abstract = {Temporally extended actions (e.g., macro actions) have proven very useful for speeding up learning, ensuring robustness and building prior knowledge into AI systems. The options framework (Precup, 2000; Sutton, Precup \& Singh, 1999) provides a natural way of incorporating such actions into reinforcement learning systems, but leaves open the issue of how good options might be identified. In this paper, we empirically explore a simple approach to creating options. The underlying assumption is that the agent will be asked to perform different goal-achievement tasks in an environment that is othertherwise the same over time. Our approach is based on the intuition that states that are frequently visited on system trajectories, could prove to be useful subgoals (e.g., McGovern \& Barto, 2001; Iba, 1989).},
	language = {en},
	booktitle = {Abstraction, {Reformulation}, and {Approximation}},
	publisher = {Springer},
	author = {Stolle, Martin and Precup, Doina},
	editor = {Koenig, Sven and Holte, Robert C.},
	year = {2002},
	pages = {212--223},
	file = {Full Text PDF:/home/javisty/Zotero/storage/PSYM4UM6/Stolle and Precup - 2002 - Learning Options in Reinforcement Learning.pdf:application/pdf},
}

@article{wan_average-reward_2021,
	title = {Average-{Reward} {Learning} and {Planning} with {Options}},
	url = {http://arxiv.org/abs/2110.13855},
	abstract = {We extend the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. Our contributions include general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of our learning algorithms. Our algorithms and convergence proofs extend those recently developed by Wan, Naik, and Sutton. We also extend the notion of option-interrupting behavior from the discounted to the average-reward formulation. We show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain.},
	urldate = {2022-05-02},
	journal = {arXiv:2110.13855 [cs]},
	author = {Wan, Yi and Naik, Abhishek and Sutton, Richard S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13855},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/AJ8EXUS9/Wan et al. - 2021 - Average-Reward Learning and Planning with Options.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/ZAYZ55LU/2110.html:text/html},
}

@inproceedings{fruit_regret_2017,
	title = {Regret {Minimization} in {MDPs} with {Options} without {Prior} {Knowledge}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
	abstract = {The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.},
	urldate = {2022-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
	year = {2017},
	file = {Full Text PDF:/home/javisty/Zotero/storage/E2ZZNB44/Fruit et al. - 2017 - Regret Minimization in MDPs with Options without P.pdf:application/pdf},
}

@inproceedings{mann_scaling_2014,
	title = {Scaling {Up} {Approximate} {Value} {Iteration} with {Options}: {Better} {Policies} with {Fewer} {Iterations}},
	shorttitle = {Scaling {Up} {Approximate} {Value} {Iteration} with {Options}},
	url = {https://proceedings.mlr.press/v32/mann14.html},
	abstract = {We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mann, Timothy and Mannor, Shie},
	month = jan,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {127--135},
	file = {Full Text PDF:/home/javisty/Zotero/storage/VL8NEW77/Mann and Mannor - 2014 - Scaling Up Approximate Value Iteration with Option.pdf:application/pdf},
}

@inproceedings{brunskill_pac-inspired_2014,
	title = {{PAC}-inspired {Option} {Discovery} in {Lifelong} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v32/brunskill14.html},
	abstract = {A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options.  This analysis helps shed light on some interesting  prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Brunskill, Emma and Li, Lihong},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {316--324},
	file = {Full Text PDF:/home/javisty/Zotero/storage/3YIZDEAQ/Brunskill and Li - 2014 - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:application/pdf},
}

@article{fruit_exploration--exploitation_2017,
	title = {Exploration--{Exploitation} in {MDPs} with {Options}},
	url = {http://arxiv.org/abs/1703.08667},
	abstract = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be {\textbackslash}textit\{provably\} much smaller than the regret suffered when learning with primitive actions.},
	urldate = {2022-05-04},
	journal = {arXiv:1703.08667 [cs, stat]},
	author = {Fruit, Ronan and Lazaric, Alessandro},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.08667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8XCSJVZL/Fruit and Lazaric - 2017 - Exploration--Exploitation in MDPs with Options.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/PUWWR2VS/1703.html:text/html},
}

@article{frans_meta_2017,
	title = {Meta {Learning} {Shared} {Hierarchies}},
	url = {http://arxiv.org/abs/1710.09767},
	abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
	urldate = {2022-05-05},
	journal = {arXiv:1710.09767 [cs]},
	author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09767},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/L6UANUQZ/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/8QBXA4PN/1710.html:text/html},
}

@article{botvinick_hierarchically_2009,
	series = {Reinforcement learning and higher cognition},
	title = {Hierarchically organized behavior and its neural foundations: {A} reinforcement learning perspective},
	volume = {113},
	issn = {0010-0277},
	shorttitle = {Hierarchically organized behavior and its neural foundations},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027708002059},
	doi = {10.1016/j.cognition.2008.08.011},
	abstract = {Research on human and animal behavior has long emphasized its hierarchical structure—the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
	language = {en},
	number = {3},
	urldate = {2022-05-06},
	journal = {Cognition},
	author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andew G.},
	month = dec,
	year = {2009},
	keywords = {Reinforcement learning, Prefrontal cortex},
	pages = {262--280},
	file = {Accepted Version:/home/javisty/Zotero/storage/SYAXBVX6/Botvinick et al. - 2009 - Hierarchically organized behavior and its neural f.pdf:application/pdf;ScienceDirect Snapshot:/home/javisty/Zotero/storage/2D8ZCJPV/S0010027708002059.html:text/html},
}

@article{solway_optimal_2014,
	title = {Optimal {Behavioral} {Hierarchy}},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003779},
	doi = {10.1371/journal.pcbi.1003779},
	abstract = {Human behavior has long been recognized to display hierarchical structure: actions fit together into subtasks, which cohere into extended goal-directed activities. Arranging actions hierarchically has well established benefits, allowing behaviors to be represented efficiently by the brain, and allowing solutions to new tasks to be discovered easily. However, these payoffs depend on the particular way in which actions are organized into a hierarchy, the specific way in which tasks are carved up into subtasks. We provide a mathematical account for what makes some hierarchies better than others, an account that allows an optimal hierarchy to be identified for any set of tasks. We then present results from four behavioral experiments, suggesting that human learners spontaneously discover optimal action hierarchies.},
	language = {en},
	number = {8},
	urldate = {2022-05-06},
	journal = {PLOS Computational Biology},
	author = {Solway, Alec and Diuk, Carlos and Córdova, Natalia and Yee, Debbie and Barto, Andrew G. and Niv, Yael and Botvinick, Matthew M.},
	month = aug,
	year = {2014},
	note = {Publisher: Public Library of Science},
	keywords = {Machine learning, Learning, Behavior, Community structure, Human learning, Information theory, Subroutines, Towns},
	pages = {e1003779},
	file = {Full Text PDF:/home/javisty/Zotero/storage/ZNE2YKX4/Solway et al. - 2014 - Optimal Behavioral Hierarchy.pdf:application/pdf;Snapshot:/home/javisty/Zotero/storage/WL286JLE/article.html:text/html},
}

@inproceedings{wen_efficiency_2020,
	title = {On {Efficiency} in {Hierarchical} {Reinforcement} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4a5cfa9281924139db466a8a19291aff-Abstract.html},
	abstract = {Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efficient solutions to sequential decision making problems, both in terms of statistical as well as computational efficiency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the benefits of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efficient HRL methods. Specifically, we formalize the intuition that HRL can exploit well repeating "subMDPs", with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efficient, as established through a finite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efficient.},
	urldate = {2022-05-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
	year = {2020},
	pages = {6708--6718},
	file = {Full Text PDF:/home/javisty/Zotero/storage/WIQFYVVL/Wen et al. - 2020 - On Efficiency in Hierarchical Reinforcement Learni.pdf:application/pdf},
}

@techreport{infante_globally_2022,
	title = {Globally {Optimal} {Hierarchical} {Reinforcement} {Learning} for {Linearly}-{Solvable} {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/2106.15380},
	abstract = {In this work we present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and the subtasks consist in moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from the non-stationarity of high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. If the set of boundary states is smaller than the entire state space, our approach can have significantly smaller sample complexity than that of a flat learner, and we validate this empirically in several experiments.},
	number = {arXiv:2106.15380},
	urldate = {2022-05-13},
	institution = {arXiv},
	author = {Infante, Guillermo and Jonsson, Anders and Gómez, Vicenç},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2106.15380},
	note = {arXiv:2106.15380 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8QBC34A3/Infante et al. - 2022 - Globally Optimal Hierarchical Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/UMAL9VDT/2106.html:text/html},
}
