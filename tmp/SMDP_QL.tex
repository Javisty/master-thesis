\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts} 

\title{Q-learning in SMDPs}

\begin{document}

\maketitle

\section{Model}

We consider a semi-Markov Decision Process $M = \langle \mathcal{S}, \mathcal{A}, p, r \rangle$ where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $p$ is the \textit{joint} transition distribution such that the model transition from $s$ to $s'$ in $\tau > 0$ time under action $a$ with probability $p(s', \tau \, | \, s, a)$, $r$ is the reward distribution such that the agent observes the (stochastic) reward $r(s, a) \in [0, 1]$ upon taking action $a$ in state $s$.

A stationary policy $\pi : \mathcal{S} \to \Delta(\mathcal{A})$ maps each state to a probability distribution over $\mathcal{A}$, as a decision rule. A deterministic policy maps each state to one action only. A history under $\pi$ is a sequence of transitions $\{s_t, a_t, r_t, \tau_t\}_{t = 0}^{+ \infty}$ such that the action $a_t$ is taken accordingly to $\pi(s_t)$ and the next state $s_{t+1}$, duration time $\tau_t$ and the reward $r_t$ are sampled from $p(\cdot, \cdot \, |\, s_t, a_t)$ and $r(s_t, a_t)$.

We place ourselves in the infinite-horizon with discount setting, meaning that we let the agent play forever, and we apply a discount based on parameter  $\gamma \in ]0, 1[$ and duration time on the received rewards, such that the total cumulative reward is defined as $\sum_{t = 0}^{+ \infty} \gamma^{\sigma_t} r_t$, where $\sigma_t = \sum_{i=0}^{t-1} \tau_i$ ($\sigma_0 = 0$), the total time at decision step $t$. To ensure the convergence of the cumulative reward, we need to assume that $\tau$ is lower-bounded by $\tau_{\min} > 0$. Indeed, if $\tau_{\min} = 0$ then one can consider the sequence of duration times $\{\tau_t = 1/(t + 1)^2\}_{t=0}^{\infty}$; it is well known that this series converges toward $\pi^2/6$, so that $\sigma_t < \pi^2/6$ for all $t$, hence $$\sum_{t = 0}^{\infty} \gamma^{\sigma_t} r_t \geq \sum_{t=0}^{\infty} \gamma^{\frac{\pi^2}{6}} = + \infty.$$

\textbf{Q-function and Bellman operator:} an interesting quantity to consider is the value function of a policy $\pi$:
$$\forall s \in \mathcal{S}, \qquad V^{\pi}(s) := \mathbb{E} \left[\sum_{t = 0}^{\infty} \gamma^{\sigma_{t}} r(s_t, a_t) \, \middle| \,  s_0 = s \right],$$
the expected discounted cumulative reward obtained when starting from state $s$ and following policy $\pi$ ($a_t \sim \pi(s_t)$). It is easy to see that, for all policies, $0 \leq V^{\pi} \leq 1/(1 - \gamma^{\tau_{\min}})$.  Similarly, the action-value (or Q-value) function of a policy $\pi$ is defined as follow:
$$\forall (s, a) \in \mathcal{S} \times \mathcal{A}, \qquad Q^{\pi}(s,a) := \mathbb{E} \left[\sum_{t = 0}^{\infty} \gamma^{\sigma_{t}} r(s_t, a_t) \, \middle| \,  s_0 = s,\, a_0 = a \right].$$

There exists an optimal policy $\pi^*$ that maximises $V^{\pi}$ and $Q^{\pi}$ over all states and state-action pairs. We denote $V^{\pi^*} = V^*$ and $Q^{\pi^*} = Q^*$.

The Q-function is the unique fixed point of the Bellman operator $\mathcal{T}: \mathbb{R}^{S \times A} \to \mathbb{R}^{S \times A}$:
$$\forall (s, a) \in \mathcal{S} \times \mathcal{A}, \quad \mathcal{T}(Q^{\pi})(s, a) := \bar r(s, a) + \mathbb{E}_{(s', t) \sim p(\cdot, \cdot | s, a)} \left[ \gamma^t Q^{\pi}(s', \pi(s')) \right] = Q^{\pi}(s, a),$$
where $\bar r(s, a)$ is the mean reward for state-action pair $(s, a)$.

In particular, the optimal Bellman operator is:
\begin{align*}
  \mathcal{T^*}(Q)(s, a) :&= \bar r(s, a) + \mathbb{E}_{(s', t) \sim p(\cdot, \cdot | s, a)} \left[ \gamma^t \max_{a' \in \mathcal{A}} Q(s', a') \right]\\
  &= \bar r(s, a) + \int_{s' \in \mathcal{S}} \int_0^{+ \infty} \gamma^t V^*(s') p(s', t | s, a) dt ds' \\
  &= \bar r(s, a) + \int_{s' \in \mathcal{S}} V^*(s') \tilde p(s' | s, a) ds'
\end{align*}

where $\tilde p(s', t | s, a) = \int_0^{+\infty} \gamma^t p(s', t | s, a) dt$ is the expected discounted probability kernel.


\section{$\gamma^{\tau}$ is subGaussian}

Let's assume that $0 < \tau$ is $\sigma^2$-subGaussian; we want to find a constant $C_{\sigma}$ such that $\gamma^\tau$ is $C_{\sigma}$-subGaussian.

First of all, here are a bunch of useful notations and relations:\\
\begin{tabular}{c c c}
  $\mu = \mathbb{E}[\tau]$ & $\eta = \mathbb{E}[\gamma^\tau]$ & $c = \frac{1}{2 \sigma^2}$\\[.3cm]
  (Jensen) $\gamma^\mu \leq \eta \iff$ & $\mu \geq \frac{\log(\eta)}{\log(\gamma)}$ & $\log(\eta + t) \leq \log(\eta) + \frac{t}{\eta}$\\[.3cm]
  $\forall t > 0$, & $\mathbb{P}(|\tau - \mu | \geq t) \leq 2 e^{-c t^2}$, & $\mathbb{E}[e^{t(\tau - \mu)}] \leq e^{\frac{1}{2}\sigma^2 t^2}$
\end{tabular}

\subsection{Main lines}

The goal is to find a constant $a>0$ such that:
$$\forall t>0, \mathbb{P}(|\gamma^\tau - \eta| \geq t) \leq 2e^{-a t^2},$$
which is sufficient to prove that $\gamma^\tau$ is $\frac{1}{\sqrt{2a}}$-subGaussian \footnote{see https://sites.ualberta.ca/~omarr/pubs/note/subgaussians.pdf, Theorem 3.1}.

First of all, given that $0 < \gamma^\tau < 1$:
$$\forall t \geq 1, \mathbb{P}(|\gamma^\tau - \eta| \geq t) = 0$$

From now on, let's assume $0 < t < 1$.

\textbf{if $\tau \leq \frac{\log(\eta)}{\log(\gamma)}$} (the following probabilities are conditional on this event):
$\tau \leq \mu$, $\gamma^\tau \geq \eta$
\begin{align}
  \mathbb{P}(|\gamma^\tau - \eta | \geq t) &= \mathbb{P}(\tau \leq \frac{\log(\eta + t)}{\log(\gamma)})\\
  &= \mathbb{P}(\mu - \tau = |\tau - \mu| \geq \mu - \frac{\log(\eta + t)}{\log(\gamma)})\\
  &\leq 2 e^{-c(\mu - \frac{\log(\eta + t)}{\log(\gamma)})^2}\\
  &\leq 2 e^{-c(\mu - \frac{\log(\eta + 1)}{\log(\gamma)})^2 t^2}
\end{align}
where the last line holds since $\mu - \frac{\log(\eta + t)}{\log(\gamma)} \geq (\mu - \frac{\log(\eta + 1)}{\log(\gamma)})t$ for all $t \in (0, 1)$ (which is showed in the next subsection).

\textbf{if $\tau \geq \mu$}:
$0 < \gamma^\tau \leq \eta$, so we only consider $0 < t < \eta$.
\begin{align}
  \mathbb{P}(|\gamma^\tau - \eta | \geq t) &= \mathbb{P}(\tau \leq \frac{\log(\eta - t)}{\log(\gamma)})\\
  &= \mathbb{P}(\tau - \mu = |\tau - \mu| \geq \mu + \frac{\log(\eta - t)}{\log(\gamma)})\\
  &\leq 2 e^{-c(\mu + \frac{\log(\eta - t)}{\log(\gamma)})^2}\\
  &\leq 2 e^{-c(\mu - \frac{\log(\eta + 1)}{\log(\gamma)})^2 t^2}
\end{align}
where the last line holds since $\mu + \frac{\log(\eta - t)}{\log(\gamma)} \geq (\mu - \frac{\log(\eta + 1)}{\log(\gamma)})t$ for all $t \in (0, 1)$, \textit{under the assumption that} $\mu \leq \frac{\log(\eta + 1) - \frac{1}{\eta}}{\log(\gamma)}$ (which is showed in the next subsection).

\textbf{if $\frac{\log(\eta)}{\log(\gamma)} \leq \tau \leq \mu$}:
$\gamma^\mu \leq \gamma^\tau \leq \eta$.
\begin{align}
  \mathbb{P}(|\gamma^\tau - \eta | \geq t) &= \mathbb{P}(\mu \geq \tau \geq \frac{\log(\eta - t)}{\log(\gamma)})\\
  &= \mathbb{P}(\mu - \tau = |\tau - \mu| \leq \mu - \frac{\log(\eta - t)}{\log(\gamma)})
\end{align}
If $t \geq \eta - \gamma^\mu$, $\mu - \frac{\log(\eta - t)}{\log(\gamma)} \leq 0$ and the preceding probability is $0$.

\subsection{Side proofs}

Let's show that:
$$\forall 0 < t < 1, (\mu - \frac{\log(\eta + t)}{\log(\gamma)})^2 \geq \kappa t^2$$ for some constant $\kappa > 0$.\\
First of all, note that $\mu > \frac{\log(\eta + t)}{\log(\gamma)}$ since $\mu \geq \frac{\log(\eta)}{\log(\gamma)} > \frac{\log(\eta + t )}{\log(\gamma)}$.\\
Denote $g(t) = (\mu - \frac{\log(\eta + t)}{\log(\gamma)})/t$, we want to show that $g$ has a minimum $\kappa > 0$ over $(0, 1)$.\\
\begin{align}
  g'(t) &= - \frac{1}{t^2}(\mu - \frac{\log(\eta + t)}{\log(\gamma)}) + \frac{1}{t}\frac{-1}{(\eta + t) \log(\gamma)}\\
  &= - \frac{1}{t^2}[\mu - \frac{\log(\eta + t)}{\log(\gamma)} + \frac{t}{(\eta + t) \log(\gamma)}]
\end{align}

To prove that $g$ is decreasing, we prove that $g'$ is non-positive by showing:
\begin{align}
  \mu &\geq \frac{\log(\eta)}{\log(\gamma)} &\geq \frac{\log(\eta + t)}{\log(\gamma)} - \frac{t}{(\eta + t)\log(\gamma)}\\
  &\iff \log(\eta) &\leq \log(\eta + t) -  \frac{t}{\eta + t} = l(t)\\
  l'(t) = \frac{1}{\eta + t} - \frac{1}{\eta + t} + \frac{t}{(\eta + t)^2} > 0
\end{align}
hence $l(t) > l(0) = \log(\eta)$, which in turn implies that $g$ is decreasing on $(0, 1)$.\\
Finally:
\begin{align}
  g(t) &\geq g(1) = \mu - \frac{\log(\eta + 1)}{\log(\gamma)}
\end{align}
which is what we wanted, with $\kappa = \mu - \frac{\log(\eta + 1)}{\log(\gamma)}$.\\

Then, let's show that:
$$\forall 0 < t < 1, \mu + \frac{\log(\eta - t)}{\log(\gamma)} \geq (\mu - \frac{\log(\eta + 1)}{\log(\gamma)}$$


\end{document}
