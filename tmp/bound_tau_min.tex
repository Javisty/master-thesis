\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts} 

\title{Q-learning in SMDPs}

\begin{document}

\maketitle

\section{Model}

We consider a semi-Markov Decision Process $M = \langle \mathcal{S}, \mathcal{A}, p, r \rangle$ where $\mathcal{S}$ is a finite set of states, $\mathcal{A}$ is a finite set of actions, $p$ is the \textit{joint} transition distribution such that the model transition from $s$ to $s'$ in $\tau > 0$ time under action $a$ with probability $p(s', \tau \, | \, s, a)$, $r$ is the reward distribution such that the agent observes the (stochastic) reward $r(s, a) \in [0, 1]$ upon taking action $a$ in state $s$.

A stationary policy $\pi : \mathcal{S} \to \Delta(\mathcal{A})$ maps each state to a probability distribution over $\mathcal{A}$, as a decision rule. A deterministic policy maps each state to one action only. A history under $\pi$ is a sequence of transitions $\{s_t, a_t, r_t, \tau_t\}_{t = 0}^{+ \infty}$ such that the action $a_t$ is taken accordingly to $\pi(s_t)$ and the next state $s_{t+1}$, duration time $\tau_t$ and the reward $r_t$ are sampled from $p(\cdot, \cdot \, |\, s_t, a_t)$ and $r(s_t, a_t)$.

We place ourselves in the infinite-horizon with discount setting, meaning that we let the agent play forever, and we apply a discount based on parameter  $\gamma \in ]0, 1[$ and duration time on the received rewards, such that the total cumulative reward is defined as $\sum_{t = 0}^{+ \infty} \gamma^{\sigma_t} r_t$, where $\sigma_t = \sum_{i=0}^{t-1} \tau_i$ ($\sigma_0 = 0$), the total time at decision step $t$. To ensure the convergence of the cumulative reward, we need to assume that $\tau$ is lower-bounded by $\tau_{\min} > 0$. Indeed, if $\tau_{\min} = 0$ then one can consider the sequence of duration times $\{\tau_t = 1/(t + 1)^2\}_{t=0}^{\infty}$; it is well known that this series converges toward $\pi^2/6$, so that $\sigma_t < \pi^2/6$ for all $t$, hence $$\sum_{t = 0}^{\infty} \gamma^{\sigma_t} r_t \geq \sum_{t=0}^{\infty} \gamma^{\frac{\pi^2}{6}} = + \infty.$$

\textbf{Q-function and Bellman operator:} an interesting quantity to consider is the value function of a policy $\pi$:
$$\forall s \in \mathcal{S}, \qquad V^{\pi}(s) := \mathbb{E} \left[\sum_{t = 0}^{\infty} \gamma^{\sigma_{t}} r(s_t, a_t) \, \middle| \,  s_0 = s \right],$$
the expected discounted cumulative reward obtained when starting from state $s$ and following policy $\pi$ ($a_t \sim \pi(s_t)$). It is easy to see that, for all policies, $0 \leq V^{\pi} \leq 1/(1 - \gamma^{\tau_{\min}})$.  Similarly, the action-value (or Q-value) function of a policy $\pi$ is defined as follow:
$$\forall (s, a) \in \mathcal{S} \times \mathcal{A}, \qquad Q^{\pi}(s,a) := \mathbb{E} \left[\sum_{t = 0}^{\infty} \gamma^{\sigma_{t}} r(s_t, a_t) \, \middle| \,  s_0 = s,\, a_0 = a \right].$$

There exists an optimal policy $\pi^*$ that maximises $V^{\pi}$ and $Q^{\pi}$ over all states and state-action pairs. We denote $V^{\pi^*} = V^*$ and $Q^{\pi^*} = Q^*$.

The Q-function is the unique fixed point of the Bellman operator $\mathcal{T}: \mathbb{R}^{S \times A} \to \mathbb{R}^{S \times A}$:
$$\forall (s, a) \in \mathcal{S} \times \mathcal{A}, \quad \mathcal{T}(Q^{\pi})(s, a) := \bar r(s, a) + \mathbb{E}_{(s', t) \sim p(\cdot, \cdot | s, a)} \left[ \gamma^t Q^{\pi}(s', \pi(s')) \right] = Q^{\pi}(s, a),$$
where $\bar r(s, a)$ is the mean reward for state-action pair $(s, a)$.

In particular, the optimal Bellman operator is:
\begin{align*}
  \mathcal{T^*}(Q)(s, a) :&= \bar r(s, a) + \mathbb{E}_{(s', t) \sim p(\cdot, \cdot | s, a)} \left[ \gamma^t \max_{a' \in \mathcal{A}} Q(s', a') \right]\\
  &= \bar r(s, a) + \int_{s' \in \mathcal{S}} \int_0^{+ \infty} \gamma^t V^*(s') p(s', t | s, a) dt ds' \\
  &= \bar r(s, a) + \int_{s' \in \mathcal{S}} V^*(s') \tilde p(s' | s, a) ds'
\end{align*}

where $\tilde p(s', t | s, a) = \int_0^{+\infty} \gamma^t p(s', t | s, a) dt$ is the expected discounted probability kernel.


\section{Bound for Q-learning in SMDP}

In this document we try to derive a sample complexity bound for Q-learning in SMDP, from the paper (Li et al., 2020). We will refer heavily to this work, as we are simply rewriting some parts of it.

The difference between the classical setting is in the discount applied to rewards: as detailed above, in SMDP the discount now depends on the total duration time of the trajectory so far. In that regard, the Q-learning update becomes:
$$Q_t(s_{t-1}, a_{t-1}) = (1 - \eta_t)Q_{t-1}(s_{t-1}, a_{t-1}) + \eta_t (r(s_{t-1}, a_{t-1}) + \gamma^{\tau_{t-2}} \max_{a\in \mathcal{A}} Q_{t-1} (s_t, a))$$
Moreover, the sample complexity we focus on concerns the number of \emph{decision epochs}, not the overall \emph{duration time}. 

We shall now go through the differences in the proof, in order to derive the corresponding bound. Before jumping into, notice that the induced Markov Chain is the same (since there's no ``time'' consideration), and so are the corresponding quantities. The only variables to change are: $t_{th}, \rho, \eta, \epsilon \in (0, \frac{1}{1 - \gamma^{\tau_{min}}}], T \geq \dots$. Also, let's denote $\gamma_t = \gamma^{\tau_{t-2}}$.\\[0.5cm]

{
(39) p.16: replace $\gamma$ by $\gamma_t$ (cf new update formula) and $Q^* = r + \tilde P V^*$
  $$\Delta_t = (I - \Lambda_t)\Delta_{t-1} + \Lambda_t(\gamma_t P_t - \tilde P)V^* + \gamma_t \Lambda_t P_t (V_{t-1} - V^*)$$
}
(40) p.16: in $\beta_{1,t}$ and $\beta_{2,t}$, $\gamma$ becomes $\gamma_i$ and sticks to $P_i$. $P$ becomes $\tilde P$.\\

(42) p.16: $$|\beta_{2,t}| \leq \gamma^{\tau_{min}} \sum_{i=1}^t \|\Delta_{i-1}\|_{\infty} \prod_{j=i+}^t (I - \Lambda_j) \Lambda_i 1$$\\

(46) p.17: unchanged.\\

(Lemma 1) p.32-33:
\begin{align*}
  (113)& \beta_{1,t} = \sum_{k=1}^{K_t(s,a)} (1-\eta)^{K_t(s,a) - k}\eta (\gamma_{t_k + 1} P_{t_k +1}(s,a) - \tilde P(s,a))V^*\\
  (114)& \letft| \sum_{k=1}^{K}(1 - \eta)^{K-k} \eta (\gamma_{t_k + 1} P_{t_k +1}(s,a) - \tilde P(s,a))V^* \right| \leq \sqrt{\eta \log(\frac{S A T}{\delta})} \gamma^{\tau_{min}}\\
  (116)& \text{ same}\\
  (117)& 0 \leq \gamma_{t_k + 1} P_{t_k +1}(s,a)V^* \leq \gamma^{\tau_{min}} \|V^*\|_\infty,\quad \text{and}\quad 0 \leq \tilde P(s,a)V^* \leq \gamma^{\tau_{min}} \|V^*\|_\infty\\
  (118)& \dots \leq \sqrt{\eta \log(\frac{SAT}{\delta})} \gamma^{\tau_{min}} \|V^*\|_\infty\\
  (44)& \tilde\tau_1 := c \gamma^{\tau_{min}} \sqrt{\eta \log(\frac{SAT}{\delta})}
\end{align*}\\

(Lemma 2) p.17: same, since no discount nor duration time (cf Markov Chain is the same) come into play.\\

(48) p.17: $|\Delta_t|  \leq \gamma^{\tau_{min}} \sum_{i=1}^{t} \|\Delta_{i-1}\|_\infty \prod_{j=i+1}^{t}(I - \Lambda_j)\Lambda_i 1 + \tilde\tau_1 \|V^*\|_\infty1 + \|\Delta_0\|_\infty 1$\\

  (49) p.17: $\|\Delta_t\|_\infty \leq \frac{\tilde\tau_1\|V^*\|_\infty + \|\Delta_0\|_\infty}{1 - \gamma^{\tau_{min}}}$\\

  (50) p.18: $|\Delta_t| \leq \frac{\gamma^{\tau_{min}}(\tilde\tau_1 \|V^*\|_\infty + \|\Delta_0\|_\infty}{1 - \gamma^{\tau_{min}}} \sum_{i=1}^{t} \prod_{j=i+1}^{t}(I-\Lambda_j)\Lambda_i 1 + \tilde\tau_1 \|V^*\|_\infty 1 + \|\Delta_0\|_\infty 1$\\

  (51b) p.18: same, which validates the induction.\\

  \textbf{(Refined analysis)} p.18: $((1-\eta)^{\frac{1}{2}t \mu_{min}} \|\Delta_0\|_\infty$ is bounded by $(1 - \gamma^{\tau_{min}})\epsilon$, with $\tilde t_h = \max \{\frac{2 \log(\frac{1}{(1-\gamma^{\tau_{min}})^2\epsilon})}{\eta \mu_{min}}, t_{frame} \} $ (same calculation, with $\gamma$ replaced by $\gamma^{\tau_{min}}$).

  Similarly, all the occurences of $\gamma$ in (Lemma 3) p.18 and its proof (Section B.3) p.34 can be replaced by $\gamma^{\tau_{min}}$ without changing the calculation.

  (Lemma 4) p.19: $w_k := (1 - \tilde\rho)^k \frac{\|\Delta_0\|_\infty}{1 - \gamma^{\tau_{min}}}$ with $\tilde\rho = (1-\gamma^{\tau_{min}})(1 - (1-(-\eta)^{\mu_{frame}}))$.
  In the proof of the lemma (section B.3) p.34, the steps can be adapted seamlessly by replacing each occurrence of $\gamma$ by $\gamma^{\tau_{min}}$ and each occurrence of $\rho$ by $\tilde\rho$.

  (Theorem 5) p.16: $\|\Delta_t\|_\infty \leq (1 - \tilde\rho)^k \frac{\|\Delta_0\|_\infty}{1 - \gamma^{\tau_{min}}} + \frac{c \gamma^{\tau_{min}}}{1 - \gamma^{\tau_{min}}} \|V^*\|_\infty \sqrt{\eta \log(\frac{SAT}{\delta})} + \epsilon$, with $k:= \max \{0, \lfloor \frac{t - \tilde t_h}{t_{frame}} \rfloor \}$.

  Doing the same process on (56), (57), (60) and (61) still keep the proof valid. We simply need to consider (58) $\tilde\eta = \min \{ \frac{(1 - \gamma^{\tau_{min}})^4 \epsilon^2}{c^2 \gamma^{2\tau_{min}} \log(\frac{SAT}{\delta})}, \frac{1}{\mu_{frame}} \}$ so that (59) $\frac{c \gamma^{\tau_{min}}}{1 - \gamma^{\tau_{min}}} \|V^*\|_\infty \sqrt{\tilde\eta \log(\frac{SAT}{\delta})} \leq \epsilon$.

  Which concludes the proof:
  

\end{document}
