
@article{szepesvari_algorithms_2010,
	title = {Algorithms for {Reinforcement} {Learning}},
	volume = {4},
	issn = {1939-4608, 1939-4616},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00268ED1V01Y201005AIM009},
	doi = {10.2200/S00268ED1V01Y201005AIM009},
	language = {en},
	number = {1},
	urldate = {2022-04-08},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Szepesvári, Csaba},
	month = jan,
	year = {2010},
	pages = {1--103},
	file = {Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf:/home/javisty/Zotero/storage/LJR5D4SR/Szepesvári - 2010 - Algorithms for Reinforcement Learning.pdf:application/pdf},
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, Massachusetts},
	edition = {Second edition},
	series = {Adaptive computation and machine learning series},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement learning},
	url = {http://www.incompleteideas.net/book/the-book.html},
	abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018},
	keywords = {Reinforcement learning},
	file = {Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:/home/javisty/Zotero/storage/PKZ9X4V6/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf:application/pdf},
}

@inproceedings{puterman_markov_1994,
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	shorttitle = {Markov {Decision} {Processes}},
	doi = {10.1002/9780470316887},
	abstract = {Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria, and explores several topics that have received little or no attention in other books. From the Publisher: 
The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic},
	booktitle = {Wiley {Series} in {Probability} and {Statistics}},
	author = {Puterman, M.},
	year = {1994},
	file = {Submitted Version:/home/javisty/Zotero/storage/3BX58TG3/Puterman - 1994 - Markov Decision Processes Discrete Stochastic Dyn.pdf:application/pdf},
}

@incollection{scholkopf_logarithmic_2007,
	title = {Logarithmic {Online} {Regret} {Bounds} for {Undiscounted} {Reinforcement} {Learning}},
	isbn = {978-0-262-25691-9},
	url = {https://direct.mit.edu/books/book/3168/chapter/87374/logarithmic-online-regret-bounds-for-undiscounted},
	abstract = {We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm’s online performance after some ﬁnite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper conﬁdence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.},
	language = {en},
	urldate = {2022-04-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {The MIT Press},
	author = {Auer, Peter and Ortner, Ronald},
	editor = {Schölkopf, Bernhard and Platt, John and Hofmann, Thomas},
	year = {2007},
	doi = {10.7551/mitpress/7503.003.0011},
	file = {Schölkopf et al. - 2007 - Logarithmic Online Regret Bounds for Undiscounted .pdf:/home/javisty/Zotero/storage/YR7VSNA7/Schölkopf et al. - 2007 - Logarithmic Online Regret Bounds for Undiscounted .pdf:application/pdf},
}

@article{jaksch_near-optimal_2010,
	title = {Near-optimal {Regret} {Bounds} for {Reinforcement} {Learning}},
	volume = {11},
	issn = {1532-4435},
	abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret Õ(DS√AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of Ω(√DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of Õ(l1/3T2/3DS√A).},
	journal = {The Journal of Machine Learning Research},
	author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	month = aug,
	year = {2010},
	pages = {1563--1600},
	file = {Full Text PDF:/home/javisty/Zotero/storage/CBC4365C/Jaksch et al. - 2010 - Near-optimal Regret Bounds for Reinforcement Learn.pdf:application/pdf},
}

@inproceedings{filippi_optimism_2010,
	title = {Optimism in reinforcement learning and {Kullback}-{Leibler} divergence},
	doi = {10.1109/ALLERTON.2010.5706896},
	abstract = {We consider model-based reinforcement learning in finite Markov Decision Processes (MDPs), focussing on so-called optimistic strategies. In MDPs, optimism can be implemented by carrying out extended value iterations under a constraint of consistency with the estimated model transition probabilities. The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this strategy, has recently been shown to guarantee near-optimal regret bounds. In this paper, we strongly argue in favor of using the Kullback-Leibler (KL) divergence for this purpose. By studying the linear maximization problem under KL constraints, we provide an efficient algorithm, termed KL-UCRL, for solving KL-optimistic extended value iteration. Using recent deviation bounds on the KL divergence, we prove that KL-UCRL provides the same guarantees as UCRL2 in terms of regret. However, numerical experiments on classical benchmarks show a significantly improved behavior, particularly when the MDP has reduced connectivity. To support this observation, we provide elements of comparison between the two algorithms based on geometric considerations.},
	booktitle = {2010 48th {Annual} {Allerton} {Conference} on {Communication}, {Control}, and {Computing} ({Allerton})},
	author = {Filippi, Sarah and Cappé, Olivier and Garivier, Aurélien},
	month = sep,
	year = {2010},
	keywords = {Algorithm design and analysis, Reinforcement learning, Markov decision processes, Benchmark testing, Context modeling, Equations, Kullback-Leibler divergence, Learning, Markov processes, Mathematical model, Model-based approaches, Optimism, Regret bounds},
	pages = {115--122},
	file = {IEEE Xplore Full Text PDF:/home/javisty/Zotero/storage/2N6YB4SE/Filippi et al. - 2010 - Optimism in reinforcement learning and Kullback-Le.pdf:application/pdf},
}

@article{bourel_tightening_2021,
	title = {Tightening {Exploration} in {Upper} {Confidence} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2004.09656},
	abstract = {The upper confidence reinforcement learning (UCRL2) algorithm introduced in (Jaksch et al., 2010) is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this algorithm and its variants have remained until now mostly theoretical as numerical experiments in simple environments exhibit long burn-in phases before the learning takes place. In pursuit of practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and (component-wise) transition distributions for each state-action pair. Furthermore, to tighten exploration, it uses an adaptive computation of the support of each transition distribution, which in turn enables us to revisit the extended value iteration procedure of UCRL2 to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments in standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable us to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear notions of local diameter and local effective support, thanks to variance-aware concentration bounds.},
	urldate = {2022-04-13},
	journal = {arXiv:2004.09656 [cs, eess, stat]},
	author = {Bourel, Hippolyte and Maillard, Odalric-Ambrym and Talebi, Mohammad Sadegh},
	month = apr,
	year = {2021},
	note = {arXiv: 2004.09656},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Appeared in Proceedings of the 27th International Conference on Machine Learning (ICML 2020). This is an improved post-proceeding version correcting minor errors},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/84PUVD3J/Bourel et al. - 2021 - Tightening Exploration in Upper Confidence Reinfor.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/S4KDNVL8/2004.html:text/html},
}

@misc{noauthor_acme_2022,
	title = {Acme: a research framework for reinforcement learning},
	copyright = {Apache-2.0},
	shorttitle = {Acme},
	url = {https://github.com/deepmind/acme},
	abstract = {A library of reinforcement learning components and agents},
	urldate = {2022-04-19},
	publisher = {DeepMind},
	month = apr,
	year = {2022},
	note = {original-date: 2020-05-01T09:18:12Z},
	keywords = {agents, reinforcement-learning, research},
}

@misc{noauthor_ucrlucrl_nodate,
	title = {{UCRL}/{UCRL} at master · {RonanFR}/{UCRL}},
	url = {https://github.com/RonanFR/UCRL},
	abstract = {Contribute to RonanFR/UCRL development by creating an account on GitHub.},
	language = {en},
	urldate = {2022-04-19},
	journal = {GitHub},
	file = {Snapshot:/home/javisty/Zotero/storage/P4TDSZKP/UCRL.html:text/html},
}

@misc{noauthor_maillard_nodate,
	title = {{MAILLARD} {Odalric} / {Average} {Reward} {Reinforcement} {Learning}},
	url = {https://gitlab.inria.fr/omaillar/average-reward-reinforcement-learning},
	abstract = {This is a code repository about Average Reward Reinforcement Learning.},
	language = {en},
	urldate = {2022-04-19},
	journal = {GitLab},
	file = {Snapshot:/home/javisty/Zotero/storage/B6EKJTSB/average-reward-reinforcement-learning.html:text/html},
}

@misc{noauthor_openaigym_2022,
	title = {openai/gym},
	url = {https://github.com/openai/gym},
	abstract = {A toolkit for developing and comparing reinforcement learning algorithms.},
	urldate = {2022-04-19},
	publisher = {OpenAI},
	month = apr,
	year = {2022},
	note = {original-date: 2016-04-27T14:59:16Z},
}

@misc{noauthor_rlberryfinite_mdppy_nodate,
	title = {rlberry/finite\_mdp.py at main · rlberry-py/rlberry},
	url = {https://github.com/rlberry-py/rlberry},
	abstract = {An easy-to-use reinforcement learning library for research and education. - rlberry/finite\_mdp.py at main · rlberry-py/rlberry},
	language = {en},
	urldate = {2022-04-19},
	journal = {GitHub},
	file = {Snapshot:/home/javisty/Zotero/storage/HRHJN62W/rlberry.html:text/html},
}

@misc{noauthor_dopamine_2022,
	title = {Dopamine},
	copyright = {Apache-2.0},
	url = {https://github.com/google/dopamine},
	abstract = {Dopamine is a research framework for fast prototyping of reinforcement learning algorithms.},
	urldate = {2022-04-19},
	publisher = {Google},
	month = apr,
	year = {2022},
	note = {original-date: 2018-07-26T09:58:36Z},
	keywords = {ai, google, ml, rl, tensorflow},
}

@misc{keng_slm_2022,
	title = {{SLM} {Lab}},
	copyright = {MIT},
	url = {https://github.com/kengz/SLM-Lab},
	abstract = {Modular Deep Reinforcement Learning framework in PyTorch. Companion library of the book "Foundations of Deep Reinforcement Learning".},
	urldate = {2022-04-19},
	author = {Keng, Wah Loon},
	month = apr,
	year = {2022},
	note = {original-date: 2017-10-02T22:20:22Z},
	keywords = {reinforcement-learning, a2c, a3c, benchmark, deep-reinforcement-learning, dqn, policy-gradient, ppo, pytorch, sac},
}

@article{dietterich_hierarchical_2000,
	title = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
	volume = {13},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10266},
	doi = {10.1613/jair.639},
	abstract = {This paper presents a new approach to hierarchical reinforcement learning based on decomposing the target Markov decision process  MDP  into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition, has both a procedural semantics{\textbar}as a subroutine hierarchy{\textbar}and a declarative semantics{\textbar}as a representation of the value function of a hierarchical policy. MAXQ uni es and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. It is based on the assumption that the programmer can identify useful subgoals and de ne subtasks that achieve these subgoals. By de ning such subgoals, the programmer constrains the set of policies that need to be considered during reinforcement learning. The MAXQ value function decomposition can represent the value function of any policy that is consistent with the given hierarchy. The decomposition also creates opportunities to exploit state abstractions, so that individual MDPs within the hierarchy can ignore large parts of the state space. This is important for the practical application of the method. This paper de nes the MAXQ hierarchy, proves formal results on its representational power, and establishes ve conditions for the safe use of state abstractions. The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges with probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the ve kinds of state abstraction. The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q  with state abstractions  converges to a recursively optimal policy much faster than at Q learning. The fact that MAXQ learns a representation of the value function has an important bene t: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration. The paper demonstrates the e ectiveness of this non-hierarchical execution experimentally. Finally, the paper concludes with a comparison to related work and a discussion of the design tradeo s in hierarchical reinforcement learning.},
	language = {en},
	urldate = {2022-04-20},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dietterich, T. G.},
	month = nov,
	year = {2000},
	pages = {227--303},
	file = {Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf:/home/javisty/Zotero/storage/NFBYX7BH/Dietterich - 2000 - Hierarchical Reinforcement Learning with the MAXQ .pdf:application/pdf},
}

@article{jong_utility_2008,
	title = {The {Utility} of {Temporal} {Abstraction} in {Reinforcement} {Learning}},
	abstract = {The hierarchical structure of real-world problems has motivated extensive research into temporal abstractions for reinforcement learning, but precisely how these abstractions allow agents to improve their learning performance is not well understood. This paper investigates the connection between temporal abstraction and an agent’s exploration policy, which determines how the agent’s performance improves over time. Experimental results with standard methods for incorporating temporal abstractions show that these methods beneﬁt learning only in limited contexts. The primary contribution of this paper is a clearer understanding of how hierarchical decompositions interact with reinforcement learning algorithms, with important consequences for the manual design or automatic discovery of action hierarchies.},
	language = {en},
	author = {Jong, Nicholas K and Hester, Todd and Stone, Peter},
	year = {2008},
	pages = {8},
	file = {Jong et al. - The Utility of Temporal Abstraction in Reinforceme.pdf:/home/javisty/Zotero/storage/E8L6RHMD/Jong et al. - The Utility of Temporal Abstraction in Reinforceme.pdf:application/pdf},
}

@inproceedings{jonsson_causal_2005,
	address = {New York, NY, USA},
	series = {{ICML} '05},
	title = {A causal approach to hierarchical decomposition of factored {MDPs}},
	isbn = {978-1-59593-180-1},
	url = {https://doi.org/10.1145/1102351.1102402},
	doi = {10.1145/1102351.1102402},
	abstract = {We present Variable Influence Structure Analysis, an algorithm that dynamically performs hierarchical decomposition of factored Markov decision processes. Our algorithm determines causal relationships between state variables and introduces temporally-extended actions that cause the values of state variables to change. Each temporally-extended action corresponds to a subtask that is significantly easier to solve than the overall task. Results from experiments show great promise in scaling to larger tasks.},
	urldate = {2022-04-20},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Jonsson, Anders and Barto, Andrew},
	month = aug,
	year = {2005},
	pages = {401--408},
	file = {Full Text PDF:/home/javisty/Zotero/storage/LZAGTATC/Jonsson and Barto - 2005 - A causal approach to hierarchical decomposition of.pdf:application/pdf},
}

@inproceedings{nachum_data-efficient_2018,
	title = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
	abstract = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
	urldate = {2022-04-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
	year = {2018},
	file = {Full Text PDF:/home/javisty/Zotero/storage/S7Q6KX88/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf:application/pdf},
}

@article{gu_q-prop_2017,
	title = {Q-{Prop}: {Sample}-{Efficient} {Policy} {Gradient} with {An} {Off}-{Policy} {Critic}},
	shorttitle = {Q-{Prop}},
	url = {http://arxiv.org/abs/1611.02247},
	abstract = {Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.},
	urldate = {2022-04-20},
	journal = {arXiv:1611.02247 [cs]},
	author = {Gu, Shixiang and Lillicrap, Timothy and Ghahramani, Zoubin and Turner, Richard E. and Levine, Sergey},
	month = feb,
	year = {2017},
	note = {arXiv: 1611.02247},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Conference Paper at the International Conference on Learning Representations (ICLR) 2017},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/W9BTLSRC/Gu et al. - 2017 - Q-Prop Sample-Efficient Policy Gradient with An O.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/9QBUXRUE/1611.html:text/html},
}

@article{precup_temporal_2000,
	title = {Temporal abstraction in reinforcement learning},
	url = {https://scholarworks.umass.edu/dissertations/AAI9978540},
	journal = {Doctoral Dissertations Available from Proquest},
	author = {Precup, Doina},
	month = jan,
	year = {2000},
	pages = {1--131},
	file = {"Temporal abstraction in reinforcement learning" by Doina Precup:/home/javisty/Zotero/storage/HGYU7BT9/AAI9978540.html:text/html},
}

@article{harb_when_2017,
	title = {When {Waiting} is not an {Option} : {Learning} {Options} with a {Deliberation} {Cost}},
	shorttitle = {When {Waiting} is not an {Option}},
	url = {http://arxiv.org/abs/1709.04571},
	abstract = {Recent work has shown that temporally extended actions (options) can be learned fully end-to-end as opposed to being specified in advance. While the problem of "how" to learn options is increasingly well understood, the question of "what" good options should be has remained elusive. We formulate our answer to what "good" options should be in the bounded rationality framework (Simon, 1957) through the notion of deliberation cost. We then derive practical gradient-based learning algorithms to implement this objective. Our results in the Arcade Learning Environment (ALE) show increased performance and interpretability.},
	urldate = {2022-04-20},
	journal = {arXiv:1709.04571 [cs]},
	author = {Harb, Jean and Bacon, Pierre-Luc and Klissarov, Martin and Precup, Doina},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.04571},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/88A9KCAL/Harb et al. - 2017 - When Waiting is not an Option  Learning Options w.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/SZP5SC3J/1709.html:text/html},
}

@article{alexander_strategic_2016,
	title = {Strategic {Attentive} {Writer} for {Learning} {Macro}-{Actions}},
	url = {http://arxiv.org/abs/1606.04695},
	abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to - i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.},
	urldate = {2022-04-20},
	journal = {arXiv:1606.04695 [cs]},
	author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04695},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8JKQ58PQ/Alexander et al. - 2016 - Strategic Attentive Writer for Learning Macro-Acti.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/R7KNETYY/1606.html:text/html},
}

@inproceedings{sutton_policy_1999,
	title = {Policy {Gradient} {Methods} for {Reinforcement} {Learning} with {Function} {Approximation}},
	volume = {12},
	url = {https://proceedings.neurips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html},
	abstract = {Function  approximation  is  essential  to  reinforcement  learning,  but  the standard approach of approximating a  value function and deter(cid:173) mining  a  policy  from  it  has so  far  proven theoretically  intractable.  In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator,  indepen(cid:173) dent of the value function,  and is  updated according to the gradient  of expected reward with respect to the policy parameters.  Williams's  REINFORCE method and actor-critic methods are examples of this  approach.  Our  main  new  result  is  to  show  that  the  gradient  can  be  written  in  a  form  suitable  for  estimation  from  experience  aided  by  an  approximate  action-value  or  advantage  function.  Using  this  result,  we  prove for  the first  time that a  version  of policy  iteration  with arbitrary differentiable function approximation is convergent to  a  locally optimal policy.},
	urldate = {2022-04-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	year = {1999},
	file = {Full Text PDF:/home/javisty/Zotero/storage/SQ88F23P/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{petrik_biasing_2008,
	title = {Biasing {Approximate} {Dynamic} {Programming} with a {Lower} {Discount} {Factor}},
	volume = {21},
	url = {https://proceedings.neurips.cc/paper/2008/hash/08c5433a60135c32e34f46a71175850c-Abstract.html},
	abstract = {Most algorithms for solving Markov decision processes rely on a discount factor, which ensures their convergence. In fact, it is often used in problems with is no intrinsic motivation. In this paper, we show that when used in approximate dynamic programming, an artificially low discount factor may significantly improve the performance on some problems, such as Tetris. We propose two explanations for this phenomenon. Our first justification follows directly from the standard approximation error bounds: using a lower discount factor may decrease the approximation error bounds. However, we also show that these bounds are loose, a thus their decrease does not entirely justify a better practical performance. We thus propose another justification: when the rewards are received only sporadically (as it is the case in Tetris), we can derive tighter bounds, which support a significant performance increase with a decrease in the discount factor.},
	urldate = {2022-04-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Petrik, Marek and Scherrer, Bruno},
	year = {2008},
	file = {Full Text PDF:/home/javisty/Zotero/storage/CPC4GFI5/Petrik and Scherrer - 2008 - Biasing Approximate Dynamic Programming with a Low.pdf:application/pdf},
}

@article{nachum_why_2019,
	title = {Why {Does} {Hierarchy} ({Sometimes}) {Work} {So} {Well} in {Reinforcement} {Learning}?},
	url = {http://arxiv.org/abs/1909.10618},
	abstract = {Hierarchical reinforcement learning has demonstrated significant success at solving difficult reinforcement learning (RL) tasks. Previous works have motivated the use of hierarchy by appealing to a number of intuitive benefits, including learning over temporally extended transitions, exploring over temporally extended periods, and training and exploring in a more semantically meaningful action space, among others. However, in fully observed, Markovian settings, it is not immediately clear why hierarchical RL should provide benefits over standard "shallow" RL architectures. In this work, we isolate and evaluate the claimed benefits of hierarchical RL on a suite of tasks encompassing locomotion, navigation, and manipulation. Surprisingly, we find that most of the observed benefits of hierarchy can be attributed to improved exploration, as opposed to easier policy learning or imposed hierarchical structures. Given this insight, we present exploration techniques inspired by hierarchy that achieve performance competitive with hierarchical RL while at the same time being much simpler to use and implement.},
	urldate = {2022-04-28},
	journal = {arXiv:1909.10618 [cs, stat]},
	author = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
	month = dec,
	year = {2019},
	note = {arXiv: 1909.10618},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented as an oral at the NeurIPS 2019 DeepRL Workshop},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/R9Y6KQ2E/Nachum et al. - 2019 - Why Does Hierarchy (Sometimes) Work So Well in Rei.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/I53ULGJY/1909.html:text/html},
}

@article{lin_self-improving_1992,
	title = {Self-improving reactive agents based on reinforcement learning, planning and teaching},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992699},
	doi = {10.1007/BF00992699},
	abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
	language = {en},
	number = {3},
	urldate = {2022-04-29},
	journal = {Machine Learning},
	author = {Lin, Long-Ji},
	month = may,
	year = {1992},
	keywords = {Reinforcement learning, connectionist networks, planning, teaching},
	pages = {293--321},
	file = {Full Text PDF:/home/javisty/Zotero/storage/IY3M38LQ/Lin - 1992 - Self-improving reactive agents based on reinforcem.pdf:application/pdf},
}

@inproceedings{stolle_learning_2002,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Options} in {Reinforcement} {Learning}},
	isbn = {978-3-540-45622-3},
	doi = {10.1007/3-540-45622-8_16},
	abstract = {Temporally extended actions (e.g., macro actions) have proven very useful for speeding up learning, ensuring robustness and building prior knowledge into AI systems. The options framework (Precup, 2000; Sutton, Precup \& Singh, 1999) provides a natural way of incorporating such actions into reinforcement learning systems, but leaves open the issue of how good options might be identified. In this paper, we empirically explore a simple approach to creating options. The underlying assumption is that the agent will be asked to perform different goal-achievement tasks in an environment that is othertherwise the same over time. Our approach is based on the intuition that states that are frequently visited on system trajectories, could prove to be useful subgoals (e.g., McGovern \& Barto, 2001; Iba, 1989).},
	language = {en},
	booktitle = {Abstraction, {Reformulation}, and {Approximation}},
	publisher = {Springer},
	author = {Stolle, Martin and Precup, Doina},
	editor = {Koenig, Sven and Holte, Robert C.},
	year = {2002},
	pages = {212--223},
	file = {Full Text PDF:/home/javisty/Zotero/storage/PSYM4UM6/Stolle and Precup - 2002 - Learning Options in Reinforcement Learning.pdf:application/pdf},
}

@article{brafman_r-max_2003,
	title = {R-max - a general polynomial time algorithm for near-optimal reinforcement learning},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303765208377},
	doi = {10.1162/153244303765208377},
	abstract = {R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
	number = {null},
	urldate = {2022-05-02},
	journal = {The Journal of Machine Learning Research},
	author = {Brafman, Ronen I. and Tennenholtz, Moshe},
	month = mar,
	year = {2003},
	keywords = {Markov decision processes, learning in games, provably efficient learning, reinforcement learning, stochastic games},
	pages = {213--231},
	file = {Full Text PDF:/home/javisty/Zotero/storage/XTJ7IW4Z/Brafman and Tennenholtz - 2003 - R-max - a general polynomial time algorithm for ne.pdf:application/pdf},
}

@article{wan_average-reward_2021,
	title = {Average-{Reward} {Learning} and {Planning} with {Options}},
	url = {http://arxiv.org/abs/2110.13855},
	abstract = {We extend the options framework for temporal abstraction in reinforcement learning from discounted Markov decision processes (MDPs) to average-reward MDPs. Our contributions include general convergent off-policy inter-option learning algorithms, intra-option algorithms for learning values and models, as well as sample-based planning variants of our learning algorithms. Our algorithms and convergence proofs extend those recently developed by Wan, Naik, and Sutton. We also extend the notion of option-interrupting behavior from the discounted to the average-reward formulation. We show the efficacy of the proposed algorithms with experiments on a continuing version of the Four-Room domain.},
	urldate = {2022-05-02},
	journal = {arXiv:2110.13855 [cs]},
	author = {Wan, Yi and Naik, Abhishek and Sutton, Richard S.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.13855},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/AJ8EXUS9/Wan et al. - 2021 - Average-Reward Learning and Planning with Options.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/ZAYZ55LU/2110.html:text/html},
}

@inproceedings{fruit_regret_2017,
	title = {Regret {Minimization} in {MDPs} with {Options} without {Prior} {Knowledge}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
	abstract = {The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.},
	urldate = {2022-05-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
	year = {2017},
	file = {Full Text PDF:/home/javisty/Zotero/storage/E2ZZNB44/Fruit et al. - 2017 - Regret Minimization in MDPs with Options without P.pdf:application/pdf},
}

@inproceedings{mann_scaling_2014,
	title = {Scaling {Up} {Approximate} {Value} {Iteration} with {Options}: {Better} {Policies} with {Fewer} {Iterations}},
	shorttitle = {Scaling {Up} {Approximate} {Value} {Iteration} with {Options}},
	url = {https://proceedings.mlr.press/v32/mann14.html},
	abstract = {We show how options, a class of control structures encompassing primitive and temporally extended actions, can play a valuable role in planning in MDPs with continuous state-spaces. Analyzing the convergence rate of Approximate Value Iteration with options reveals that for pessimistic initial value function estimates, options can speed up convergence compared to planning with only primitive actions even when the temporally extended actions are suboptimal and sparsely scattered throughout the state-space. Our experimental results in an optimal replacement task and a complex inventory management task demonstrate the potential for options to speed up convergence in practice. We show that options induce faster convergence to the optimal value function, which implies deriving better policies with fewer iterations.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Mann, Timothy and Mannor, Shie},
	month = jan,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {127--135},
	file = {Full Text PDF:/home/javisty/Zotero/storage/VL8NEW77/Mann and Mannor - 2014 - Scaling Up Approximate Value Iteration with Option.pdf:application/pdf},
}

@inproceedings{brunskill_pac-inspired_2014,
	title = {{PAC}-inspired {Option} {Discovery} in {Lifelong} {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v32/brunskill14.html},
	abstract = {A key goal of AI is to create lifelong learning agents that can leverage prior experience to improve performance on later tasks. In reinforcement-learning problems, one way to summarize prior experience for future use is through options, which are temporally extended actions (subpolicies) for how to behave. Options can then be used to potentially accelerate learning in new reinforcement learning tasks. In this work, we provide the first formal analysis of the sample complexity, a measure of learning speed, of reinforcement learning with options.  This analysis helps shed light on some interesting  prior empirical results on when and how options may accelerate learning. We then quantify the benefit of options in reducing sample complexity of a lifelong learning agent. Finally, the new theoretical insights inspire a novel option-discovery algorithm that aims at minimizing overall sample complexity in lifelong reinforcement learning.},
	language = {en},
	urldate = {2022-05-04},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Brunskill, Emma and Li, Lihong},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {316--324},
	file = {Full Text PDF:/home/javisty/Zotero/storage/3YIZDEAQ/Brunskill and Li - 2014 - PAC-inspired Option Discovery in Lifelong Reinforc.pdf:application/pdf},
}

@article{fruit_exploration--exploitation_2017,
	title = {Exploration--{Exploitation} in {MDPs} with {Options}},
	url = {http://arxiv.org/abs/1703.08667},
	abstract = {While a large body of empirical results show that temporally-extended actions and options may significantly affect the learning performance of an agent, the theoretical understanding of how and when options can be beneficial in online reinforcement learning is relatively limited. In this paper, we derive an upper and lower bound on the regret of a variant of UCRL using options. While we first analyze the algorithm in the general case of semi-Markov decision processes (SMDPs), we show how these results can be translated to the specific case of MDPs with options and we illustrate simple scenarios in which the regret of learning with options can be {\textbackslash}textit\{provably\} much smaller than the regret suffered when learning with primitive actions.},
	urldate = {2022-05-04},
	journal = {arXiv:1703.08667 [cs, stat]},
	author = {Fruit, Ronan and Lazaric, Alessandro},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.08667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8XCSJVZL/Fruit and Lazaric - 2017 - Exploration--Exploitation in MDPs with Options.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/PUWWR2VS/1703.html:text/html},
}

@article{strehl_reinforcement_2009,
	title = {Reinforcement {Learning} in {Finite} {MDPs}: {PAC} {Analysis}},
	volume = {10},
	issn = {1532-4435},
	shorttitle = {Reinforcement {Learning} in {Finite} {MDPs}},
	abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These "PAC-MDP" algorithms include the well-known E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
	journal = {The Journal of Machine Learning Research},
	author = {Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
	month = dec,
	year = {2009},
	pages = {2413--2444},
	file = {Full Text PDF:/home/javisty/Zotero/storage/M3ETWF3E/Strehl et al. - 2009 - Reinforcement Learning in Finite MDPs PAC Analysi.pdf:application/pdf},
}

@article{frans_meta_2017,
	title = {Meta {Learning} {Shared} {Hierarchies}},
	url = {http://arxiv.org/abs/1710.09767},
	abstract = {We develop a metalearning approach for learning hierarchically structured policies, improving sample efficiency on unseen tasks through the use of shared primitives---policies that are executed for large numbers of timesteps. Specifically, a set of primitives are shared within a distribution of tasks, and are switched between by task-specific policies. We provide a concrete metric for measuring the strength of such hierarchies, leading to an optimization problem for quickly reaching high reward on unseen tasks. We then present an algorithm to solve this problem end-to-end through the use of any off-the-shelf reinforcement learning method, by repeatedly sampling new tasks and resetting task-specific policies. We successfully discover meaningful motor primitives for the directional movement of four-legged robots, solely by interacting with distributions of mazes. We also demonstrate the transferability of primitives to solve long-timescale sparse-reward obstacle courses, and we enable 3D humanoid robots to robustly walk and crawl with the same policy.},
	urldate = {2022-05-05},
	journal = {arXiv:1710.09767 [cs]},
	author = {Frans, Kevin and Ho, Jonathan and Chen, Xi and Abbeel, Pieter and Schulman, John},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09767},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/L6UANUQZ/Frans et al. - 2017 - Meta Learning Shared Hierarchies.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/8QBXA4PN/1710.html:text/html},
}

@inproceedings{wen_efficiency_2020,
	title = {On {Efficiency} in {Hierarchical} {Reinforcement} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4a5cfa9281924139db466a8a19291aff-Abstract.html},
	abstract = {Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efficient solutions to sequential decision making problems, both in terms of statistical as well as computational efficiency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the benefits of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efficient HRL methods. Specifically, we formalize the intuition that HRL can exploit well repeating "subMDPs", with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efficient, as established through a finite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efficient.},
	urldate = {2022-05-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
	year = {2020},
	pages = {6708--6718},
	file = {Full Text PDF:/home/javisty/Zotero/storage/WIQFYVVL/Wen et al. - 2020 - On Efficiency in Hierarchical Reinforcement Learni.pdf:application/pdf},
}

@inproceedings{strehl_pac_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {{PAC} model-free reinforcement learning},
	isbn = {978-1-59593-383-6},
	url = {https://doi.org/10.1145/1143844.1143955},
	doi = {10.1145/1143844.1143955},
	abstract = {For a Markov Decision Process with finite state (size S) and action spaces (size A per state), we propose a new algorithm---Delayed Q-Learning. We prove it is PAC, achieving near optimal performance except for Õ(SA) timesteps using O(SA) space, improving on the Õ(S2 A) bounds of best previous algorithms. This result proves efficient reinforcement learning is possible without learning a model of the MDP from experience. Learning takes place from a single continuous thread of experience---no resets nor parallel sampling is used. Beyond its smaller storage and experience requirements, Delayed Q-learning's per-experience computation cost is much less than that of previous PAC algorithms.},
	urldate = {2022-05-10},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
	month = jun,
	year = {2006},
	pages = {881--888},
	file = {Full Text PDF:/home/javisty/Zotero/storage/RZESZR34/Strehl et al. - 2006 - PAC model-free reinforcement learning.pdf:application/pdf},
}

@inproceedings{even-dar_convergence_2001,
	title = {Convergence of {Optimistic} and {Incremental} {Q}-{Learning}},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper/2001/hash/6f2688a5fce7d48c8d19762b88c32c3b-Abstract.html},
	abstract = {Vie sho,v the convergence of tV/O deterministic variants of Q(cid:173) learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an E(cid:173) optimal policy. The second is a new and novel algorithm incremen(cid:173) tal Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algo(cid:173) rithm can be viewed as derandomization of the E-greedy Q-learning.},
	urldate = {2022-05-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Even-dar, Eyal and Mansour, Yishay},
	year = {2001},
	file = {Full Text PDF:/home/javisty/Zotero/storage/YEKW2593/Even-dar and Mansour - 2001 - Convergence of Optimistic and Incremental Q-Learni.pdf:application/pdf},
}

@techreport{infante_globally_2022,
	title = {Globally {Optimal} {Hierarchical} {Reinforcement} {Learning} for {Linearly}-{Solvable} {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/2106.15380},
	abstract = {In this work we present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and the subtasks consist in moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from the non-stationarity of high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. If the set of boundary states is smaller than the entire state space, our approach can have significantly smaller sample complexity than that of a flat learner, and we validate this empirically in several experiments.},
	number = {arXiv:2106.15380},
	urldate = {2022-05-13},
	institution = {arXiv},
	author = {Infante, Guillermo and Jonsson, Anders and Gómez, Vicenç},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2106.15380},
	note = {arXiv:2106.15380 [cs]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/8QBC34A3/Infante et al. - 2022 - Globally Optimal Hierarchical Reinforcement Learni.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/UMAL9VDT/2106.html:text/html},
}

@techreport{wei_model-free_2020,
	title = {Model-free {Reinforcement} {Learning} in {Infinite}-horizon {Average}-reward {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1910.07072},
	abstract = {Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{2/3\})\$ regret after \$T\$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{T\})\$, albeit with a stronger ergodic assumption. This result significantly improves over the \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{3/4\})\$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019a) for ergodic MDPs in the infinite-horizon average-reward setting.},
	number = {arXiv:1910.07072},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
	month = feb,
	year = {2020},
	doi = {10.48550/arXiv.1910.07072},
	note = {arXiv:1910.07072 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/WK4ZL7KQ/Wei et al. - 2020 - Model-free Reinforcement Learning in Infinite-hori.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/U2PML35K/1910.html:text/html},
}

@techreport{dong_q-learning_2019,
	title = {Q-learning with {UCB} {Exploration} is {Sample} {Efficient} for {Infinite}-{Horizon} {MDP}},
	url = {http://arxiv.org/abs/1901.09311},
	abstract = {A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. {\textbackslash}cite\{jin2018q\} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards {\textbackslash}emph\{without\} accessing a generative model. We show that the {\textbackslash}textit\{sample complexity of exploration\} of our algorithm is bounded by \${\textbackslash}tilde\{O\}(\{{\textbackslash}frac\{SA\}\{{\textbackslash}epsilon{\textasciicircum}2(1-{\textbackslash}gamma){\textasciicircum}7\}\})\$. This improves the previously best known result of \${\textbackslash}tilde\{O\}(\{{\textbackslash}frac\{SA\}\{{\textbackslash}epsilon{\textasciicircum}4(1-{\textbackslash}gamma){\textasciicircum}8\}\})\$ in this setting achieved by delayed Q-learning {\textbackslash}cite\{strehl2006pac\}, and matches the lower bound in terms of \${\textbackslash}epsilon\$ as well as \$S\$ and \$A\$ except for logarithmic factors.},
	number = {arXiv:1901.09311},
	urldate = {2022-06-13},
	institution = {arXiv},
	author = {Dong, Kefan and Wang, Yuanhao and Chen, Xiaoyu and Wang, Liwei},
	month = sep,
	year = {2019},
	doi = {10.48550/arXiv.1901.09311},
	note = {arXiv:1901.09311 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/V3JV8D7K/Dong et al. - 2019 - Q-learning with UCB Exploration is Sample Efficien.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/H3CHNIYN/1901.html:text/html},
}

@inproceedings{besson_aggregation_2018,
	title = {Aggregation of {Multi}-{Armed} {Bandits} {Learning} {Algorithms} for {Opportunistic} {Spectrum} {Access}},
	url = {https://hal.inria.fr/hal-01705292},
	doi = {10.1109/wcnc.2018.8377070},
	abstract = {Multi-armed bandit algorithms have been recently studied and evaluated for Cognitive Radio (CR), especially in the context of Opportunistic Spectrum Access (OSA). Several solutions have been explored based on various models, but it is hard to exactly predict which could be the best for real-world conditions at every instants. Hence, expert aggregation algorithms can be useful to select on the run the best algorithm for a specific situation. Aggregation algorithms, such as Exp4 dating back from 2002, have never been used for OSA learning, and we show that it appears empirically sub-efficient when applied to simple stochastic problems. In this article, we present an improved variant, called Aggregator. For synthetic OSA problems modeled as Multi-Armed Bandit (MAB) problems, simulation results are presented to demonstrate its empirical efficiency. We combine classical algorithms, such as Thompson sampling, Upper-Confidence Bounds algorithms (UCB and variants), and Bayesian or Kullback-Leibler UCB. Our algorithm offers good performance compared to state-of-the-art algorithms (Exp4, CORRAL or LearnExp), and appears as a robust approach to select on the run the best algorithm for any stochastic MAB problem, being more realistic to real-world radio settings than any tuning-based approach.},
	language = {en},
	urldate = {2022-06-13},
	author = {Besson, Lilian and Kaufmann, Emilie and Moy, Christophe},
	month = apr,
	year = {2018},
	file = {Full Text PDF:/home/javisty/Zotero/storage/LB2LE8UB/Besson et al. - 2018 - Aggregation of Multi-Armed Bandits Learning Algori.pdf:application/pdf;Snapshot:/home/javisty/Zotero/storage/NFBARXPX/hal-01705292.html:text/html},
}

@techreport{azar_minimax_2017,
	title = {Minimax {Regret} {Bounds} for {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1703.05449},
	abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \${\textbackslash}tilde\{O\}( {\textbackslash}sqrt\{HSAT\} + H{\textasciicircum}2S{\textasciicircum}2A+H{\textbackslash}sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \${\textbackslash}tilde\{O\}(HS {\textbackslash}sqrt\{AT\})\$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when \$T{\textbackslash}geq H{\textasciicircum}3S{\textasciicircum}3A\$ and \$SA{\textbackslash}geq H\$, it leads to a regret of \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{HSAT\})\$ that matches the established lower bound of \${\textbackslash}Omega({\textbackslash}sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based "exploration bonuses" that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
	language = {en},
	number = {arXiv:1703.05449},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, Rémi},
	month = jul,
	year = {2017},
	note = {arXiv:1703.05449 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Azar et al. - 2017 - Minimax Regret Bounds for Reinforcement Learning.pdf:/home/javisty/Zotero/storage/J3ND9TA4/Azar et al. - 2017 - Minimax Regret Bounds for Reinforcement Learning.pdf:application/pdf},
}

@techreport{dann_unifying_2018,
	title = {Unifying {PAC} and {Regret}: {Uniform} {PAC} {Bounds} for {Episodic} {Reinforcement} {Learning}},
	shorttitle = {Unifying {PAC} and {Regret}},
	url = {http://arxiv.org/abs/1703.07710},
	abstract = {Statistical performance bounds for reinforcement learning (RL) algorithms can be critical for high-stakes applications like healthcare. This paper introduces a new framework for theoretically measuring the performance of such algorithms called Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct (PAC) framework. In contrast to the PAC framework, the uniform version may be used to derive high probability regret guarantees and so forms a bridge between the two setups that has been missing in the literature. We demonstrate the beneﬁts of the new framework for ﬁnite-state episodic MDPs with a new algorithm that is Uniform-PAC and simultaneously achieves optimal regret and PAC guarantees except for a factor of the horizon.},
	language = {en},
	number = {arXiv:1703.07710},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
	month = jan,
	year = {2018},
	note = {arXiv:1703.07710 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: appears in Neural Information Processing Systems 2017},
	file = {Dann et al. - 2018 - Unifying PAC and Regret Uniform PAC Bounds for Ep.pdf:/home/javisty/Zotero/storage/MHIPGHU6/Dann et al. - 2018 - Unifying PAC and Regret Uniform PAC Bounds for Ep.pdf:application/pdf},
}

@techreport{jin_is_2018,
	title = {Is {Q}-learning {Provably} {Efficient}?},
	url = {http://arxiv.org/abs/1807.03765},
	abstract = {Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more ﬂexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [7, 22]. The theoretical question of “whether model-free algorithms can be made sample eﬃcient” is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with ﬁnitely many states and actions.},
	language = {en},
	number = {arXiv:1807.03765},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.03765 [cs, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Artificial Intelligence},
	annote = {Comment: Best paper in ICML 2018 workshop "Exploration in RL"},
	file = {Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf:/home/javisty/Zotero/storage/UVTMRF66/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf:application/pdf},
}

@article{kearns_near-optimal_2002,
	title = {Near-{Optimal} {Reinforcement} {Learning} in {Polynomial} {Time}},
	volume = {49},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1017984413808},
	doi = {10.1023/A:1017984413808},
	abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
	language = {en},
	number = {2},
	urldate = {2022-06-27},
	journal = {Machine Learning},
	author = {Kearns, Michael and Singh, Satinder},
	month = nov,
	year = {2002},
	keywords = {Markov decision processes, reinforcement learning, exploration versus exploitation},
	pages = {209--232},
	file = {Full Text PDF:/home/javisty/Zotero/storage/UQWUP5KS/Kearns and Singh - 2002 - Near-Optimal Reinforcement Learning in Polynomial .pdf:application/pdf},
}

@techreport{li_sample_2021,
	title = {Sample {Complexity} of {Asynchronous} {Q}-{Learning}: {Sharper} {Analysis} and {Variance} {Reduction}},
	shorttitle = {Sample {Complexity} of {Asynchronous} {Q}-{Learning}},
	url = {http://arxiv.org/abs/2006.03041},
	abstract = {Asynchronous Q-learning aims to learn the optimal action-value function (or Q-function) of a Markov decision process (MDP), based on a single trajectory of Markovian samples induced by a behavior policy. Focusing on a \${\textbackslash}gamma\$-discounted MDP with state space \${\textbackslash}mathcal\{S\}\$ and action space \${\textbackslash}mathcal\{A\}\$, we demonstrate that the \${\textbackslash}ell\_\{{\textbackslash}infty\}\$-based sample complexity of classical asynchronous Q-learning --- namely, the number of samples needed to yield an entrywise \${\textbackslash}varepsilon\$-accurate estimate of the Q-function --- is at most on the order of \${\textbackslash}frac\{1\}\{{\textbackslash}mu\_\{{\textbackslash}min\}(1-{\textbackslash}gamma){\textasciicircum}5{\textbackslash}varepsilon{\textasciicircum}2\}+ {\textbackslash}frac\{t\_\{mix\}\}\{{\textbackslash}mu\_\{{\textbackslash}min\}(1-{\textbackslash}gamma)\}\$ up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here, \$t\_\{mix\}\$ and \${\textbackslash}mu\_\{{\textbackslash}min\}\$ denote respectively the mixing time and the minimum state-action occupancy probability of the sample trajectory. The first term of this bound matches the sample complexity in the synchronous case with independent samples drawn from the stationary distribution of the trajectory. The second term reflects the cost taken for the empirical distribution of the Markovian trajectory to reach a steady state, which is incurred at the very beginning and becomes amortized as the algorithm runs. Encouragingly, the above bound improves upon the state-of-the-art result {\textbackslash}cite\{qu2020finite\} by a factor of at least \${\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}\$ for all scenarios, and by a factor of at least \$t\_\{mix\}{\textbar}{\textbackslash}mathcal\{S\}{\textbar}{\textbar}{\textbackslash}mathcal\{A\}{\textbar}\$ for any sufficiently small accuracy level \${\textbackslash}varepsilon\$. Further, we demonstrate that the scaling on the effective horizon \${\textbackslash}frac\{1\}\{1-{\textbackslash}gamma\}\$ can be improved by means of variance reduction.},
	language = {en},
	number = {arXiv:2006.03041},
	urldate = {2022-07-06},
	institution = {arXiv},
	author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
	month = aug,
	year = {2021},
	note = {arXiv:2006.03041 [cs, eess, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
	annote = {Comment: accepted in part to Neural Information Processing Systems (NeurIPS) 2020},
	file = {Li et al. - 2021 - Sample Complexity of Asynchronous Q-Learning Shar.pdf:/home/javisty/Zotero/storage/WDPV3UUX/Li et al. - 2021 - Sample Complexity of Asynchronous Q-Learning Shar.pdf:application/pdf},
}

@phdthesis{fruit_thesis_2019,
author = {Fruit, Ronan},
year = {2019},
month = {11},
pages = {},
title = {Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge}
}

@article{bellman1957markovian,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@article{blackwell1962discrete,
  title={Discrete dynamic programming},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={719--726},
  year={1962},
  publisher={JSTOR}
}

@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  publisher={King's College, Cambridge United Kingdom}
}

@article{jaakkola1994reinforcement,
  title={Reinforcement learning algorithm for partially observable Markov decision problems},
  author={Jaakkola, Tommi and Singh, Satinder and Jordan, Michael},
  journal={Advances in neural information processing systems},
  volume={7},
  year={1994}
}

@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
