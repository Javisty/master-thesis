\chapter{Conclusion}
\label{Chapter5}

Going for temporal abstraction for Reinforcement Learning can be a tricky decision to take, as its effects on the training and exploration are not fully understood, leading to risks of worsening performances, contrarily to robust, reliable methods already present in classical RL. This observation holds even though many approaches have already been proposed and successfully validated by experiments, because experiments do not cover all the possible cases, and thus give no real guarantee. That is the role of theoretical analysis, of performance measures; there already exist plenty for classical algorithms, however they are lacking in the case of temporal abstraction. Hence this thesis, by providing a sample complexity bound for the most famous temporally abstracted method, makes a significant contribution to this field and gives more insight about its perks. Hopefully this is a stepping stone towards a deeper understanding of temporal abstraction. Notably, one question has been answered but several have been raised, all leading to this final goal.
